{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "css_setup",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cW8VKkCJ1LceIb31gr2S",
   "metadata": {},
   "source": [
    "# Bias-variance and regularization\n",
    "This exercise is about applying regularization to mitigate the effects of overfitting. This exercise assumes that you have read the tutorial about cross validation\n",
    ".\n",
    "\n",
    "<article class=\"message\">\n",
    "    <div class=\"message-body\">\n",
    "        <strong>List of tasks</strong>\n",
    "        <ul style=\"list-style: none;\">\n",
    "            <li>\n",
    "            <a href=\"#reflect\">Task 1: Tutorial review</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#ridge0\">Task 2: Reflections on regularization</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#ridge1111\">Task 3: Loading the dataset</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#ride\">Task 4: Implementing regularization</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#eval\">Task 5: Evaluating models</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#cv\">Task 6: Cross validation</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#reflection\">Task 7: Reflection on results</a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</article>\n",
    "\n",
    "## Reflection on the tutorial\n",
    "\n",
    "---\n",
    "**Task 1 (easy): Tutorial reviewüë©‚Äçüíªüí°**\n",
    "1. Make a copy of the tutorial and make edits in the copy. \n",
    "\n",
    "2. In the tutorial, go to the \"Hold-out validation\" section and add a for loop that runs the cell for at least 10 iterations. That is, in each iteration:\n",
    "    - Run the hold-out train-validation split. \n",
    "    - Fit the model on the training set. \n",
    "    - Compute and store the $R^2$ scores on the validation set.\n",
    "\n",
    "\n",
    "3. Inspect the minimum and maximum $R^2$ scores and calculate their mean and variance. What does this indicate about the influence of the training set on model predictions?\n",
    "\n",
    "4. Go to the \"Effects of polynomials on model fit\" section and implement 10 fold cross validation to train the models with 3rd, 4th, and 5th order polynomials. Does this affect the fit of the models? \n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "q2iBTt8SjxGUCk9oRRWa_",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your reflections here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TJndBGd3qMAy_R6kTIEA",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "In the cross validation tutorial\n",
    ", it was observed that adding third or higher order polynomial terms results in overfitting of the regression model. In the following steps, a model pipeline similar to the one from the tutorial will be built, this time using ridge regression.\n",
    "\n",
    "---\n",
    "**Task 2 (easy): Reflections on regularizationüí°**\n",
    "1. Define the loss function used in ridge regression.\n",
    "2. What is the importance of the regularization parameter $\\lambda$?\n",
    "3. What influence does $\\lambda$ have when it becomes:    - 0?\n",
    "    - 1?\n",
    "    - Large?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc08309",
   "metadata": {},
   "source": [
    "## Task 2 reflection\n",
    "\n",
    "\n",
    "#### **1. Loss function used in Ridge Regression**\n",
    "Loss function = \n",
    "The sum of squared residuals +\n",
    "Œª * slope2\t\t\t\t\t        (ridge regression penalty)\n",
    "\n",
    "- First term: Least squares, ie. Mean squared error (MSE).\n",
    "- Second term: \\( L2 \\)-norm penalty (ridge regression penalty)\n",
    "\n",
    "#### **2. Importance of \\( Œª \\)**\n",
    "- Balances **fit** (training error) vs **simplicity** (smaller coefficients).\n",
    "- Optimizing \\( Œª \\) reduces overfitting and improves generalization.\n",
    "\n",
    "#### **3. Influence of \\( Œª \\)**\n",
    "- **\\( Œª = 0 \\):** No penalty, behaves like linear regression (may overfit).\n",
    "- **\\( Œª = 1 \\):** Moderate penalty, reduces overfitting slightly.\n",
    "- **\\( Œª \\) large:** Strong penalty, shrinks coefficients, risk of underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y15cKmKV3ozIxHNepM1vY",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 3 (easy): Loading the datasetüë©‚Äçüíª**\n",
    "1. Run the cell below to import libraries and set up the dataset.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "Ix6U04WvbMXlUsdSlB1r6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge # additional import for regularization\n",
    "\n",
    "np.random.seed(99) # seed for randomization \n",
    "\n",
    "dataset = fetch_california_housing(as_frame=True)\n",
    "\n",
    "df = dataset.frame # This is the dataframe (a table)\n",
    "\n",
    "X = dataset.data # These are the input features (anything but the house price)\n",
    "y = dataset.target # This contains the output features (just the house price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oKoyEnezSNFZoUCxd9Rxf",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 4 (medium): Implementing regularizationüë©‚Äçüíª**\n",
    "1. Run the cell below to:    - create a third-order polynomial model with ridge regression using the `Ridge`\n",
    " class from Scikit learn.\n",
    "    - use the `np.geomspace`\n",
    " function to create an array, `regularization_params`\n",
    ", with values exponentially spaced between $10^{-10}$ and $10^2$. These values will be used to vary the regularization parameter. \n",
    "\n",
    "\n",
    "2. In the cell below, divide the dataset into an 80-20 training-validation split and use the training set to train third-order Ridge regression models with different regularization parameters $\\lambda_i$, by iterating over the elements in `regularization_params`\n",
    ". \n",
    "\n",
    "**Note:** Note: the regularization parameter $\\lambda$  is called alpha in sckit learn.\n",
    "\n",
    "3. Asses the performance of the models on the validation set by calculating the $R^2$ scores and store them in `scores`\n",
    ".\n",
    "\n",
    "4. Run the cell below to plot the $R^2$ scores for each model (each regularization value). What does the plot reveal about the effect of the regularization parameter on the perfomance of the model on the testing set. \n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "mCI_86aNYcKycQd4i1kKH",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization parameters\n",
    "regularization_params = np.geomspace(1e-10, 1e2, 20)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize an empty list to store R-squared scores\n",
    "scores = []\n",
    "\n",
    "# Train Ridge regression models with different regularization parameters\n",
    "for alpha in regularization_params:\n",
    "    model = Pipeline([\n",
    "        (\"features\", PolynomialFeatures(3)),  # 3rd-order polynomial features\n",
    "        (\"normalization\", Normalizer()),      # Normalize features\n",
    "        (\"model\", Ridge(alpha=alpha))         # Ridge regression with current alpha\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    r2_score = model.score(X_val, y_val)     # Evaluate on validation set\n",
    "    scores.append(r2_score)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(regularization_params, scores, marker='o', linestyle='-', label=\"Validation $R^2$\")\n",
    "plt.xscale('log')\n",
    "plt.title('R-squared Scores vs Regularization Parameters ($\\\\lambda$)')\n",
    "plt.xlabel('Regularization Parameter ($\\\\lambda$)')\n",
    "plt.ylabel('R-squared Score')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fieoXEGRnGnb1bi6wyHp9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 5 (easy): Evaluating modelsüë©‚Äçüíª**\n",
    "This task is about evaluating the effects of the regularisation parameters.\n",
    "1. In the cell above, add a for-loop to rerun the cell 20 times and store the $R^2$ results from each iteration. The loop should repeat the 80-20 hold-out train-validation split each time as in [Task 1](#reflect). \n",
    "2. Calculate the mean and variance of the $R^2$ scores for each regularization value then run the cell below to plot the results. \n",
    "3. Based on the plots, which regularization parameter value gives the best results and why? Note down your observations and reflections in the cell below as it will be used in the next task.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "GDoRPfCgNO_HBxUuYc5A0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Regularization parameters\n",
    "regularization_params = np.geomspace(1e-10, 1e2, 20)\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 20\n",
    "\n",
    "# Initialize lists to store mean and variance of R^2 scores for each parameter\n",
    "mean_scores = []\n",
    "variance_scores = []\n",
    "\n",
    "# Loop through each regularization parameter\n",
    "for alpha in regularization_params:\n",
    "    r2_scores = []  # Store R^2 scores for this parameter\n",
    "    \n",
    "    # Repeat hold-out validation 20 times\n",
    "    for i in range(num_iterations):\n",
    "        # Split dataset into train and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "        \n",
    "        # Create and train the model\n",
    "        model = Pipeline([\n",
    "            (\"features\", PolynomialFeatures(3)),\n",
    "            (\"normalization\", Normalizer()),\n",
    "            (\"model\", Ridge(alpha=alpha))\n",
    "        ])\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute R^2 score and store\n",
    "        r2_scores.append(model.score(X_val, y_val))\n",
    "    \n",
    "    # Calculate and store mean and variance for this parameter\n",
    "    mean_scores.append(np.mean(r2_scores))\n",
    "    variance_scores.append(np.var(r2_scores))\n",
    "\n",
    "# Plot the mean and variance R-squared scores\n",
    "plt.figure(figsize=(10, 5))  # Set the figure size\n",
    "\n",
    "# Subplot for Mean R-squared\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(regularization_params, mean_scores, marker='o', linestyle='-', label='Mean $R^2$')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Parameter ($\\\\lambda$)')\n",
    "plt.ylabel('Mean $R^2$')\n",
    "plt.title('Mean R-squared Scores')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot for Variance\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(regularization_params, variance_scores, marker='o', linestyle='-', label='Variance')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Parameter ($\\\\lambda$)')\n",
    "plt.ylabel('Variance of $R^2$')\n",
    "plt.title('Variance of R-squared Scores')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Ensure proper spacing between subplots\n",
    "plt.show()\n",
    "\n",
    "# Reflection:\n",
    "# The optimal regularization parameter minimizes variance while maintaining a high mean R^2 score.\n",
    "# Typically, moderate values of lambda achieve the best balance, avoiding overfitting (low variance) and underfitting (high mean R^2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NQ5hPVfDodKJtaKv1s2fB",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "---\n",
    "**Task 6 (medium): Cross validationüë©‚Äçüíª**\n",
    "This task investigates model generalization using k-fold cross validation.\n",
    "1. Construct a new model, with the same setup as before by using the optimal regularization parameter found in the previous task. \n",
    "2. Train the model using k-fold cross validation. Set the number of folds to 2.\n",
    "3. Vary the number of folds from 2 to 20 and calculate the mean and the standard deviation of the $R^2$ score for each fold. \n",
    "4. Plot the mean and the standard deviation of the $R^2$  scores as a function of the folds.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "5bq6KuV5aOZFMV6Gy-YZP",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, Normalizer\n",
    "\n",
    "# Optimal regularization parameter (replace with the best from Task 5)\n",
    "optimal_alpha = regularization_params[np.argmax(mean_scores)]  # Example selection logic\n",
    "\n",
    "# Define number of folds to vary\n",
    "folds_range = range(2, 21)\n",
    "\n",
    "# Initialize lists to store mean and standard deviation of R^2 scores\n",
    "mean_r2_scores = []\n",
    "std_r2_scores = []\n",
    "\n",
    "# Loop through each fold count\n",
    "for n_folds in folds_range:\n",
    "    # Create k-fold cross-validator\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Define model\n",
    "    model = Pipeline([\n",
    "        (\"features\", PolynomialFeatures(3)),\n",
    "        (\"normalization\", Normalizer()),\n",
    "        (\"model\", Ridge(alpha=optimal_alpha))\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation and collect scores\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "    \n",
    "    # Calculate mean and standard deviation of R^2 scores\n",
    "    mean_r2_scores.append(np.mean(scores))\n",
    "    std_r2_scores.append(np.std(scores))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot mean R-squared scores\n",
    "plt.plot(folds_range, mean_r2_scores, marker='o', label=\"Mean $R^2$\", linestyle='-')\n",
    "\n",
    "# Plot standard deviation of R-squared scores\n",
    "plt.plot(folds_range, std_r2_scores, marker='o', label=\"Standard Deviation\", linestyle='--')\n",
    "\n",
    "plt.title(\"Cross-Validation Performance with Varying Folds\")\n",
    "plt.xlabel(\"Number of Folds\")\n",
    "plt.ylabel(\"$R^2$ Score / Standard Deviation\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Reflection:\n",
    "# - Higher folds tend to reduce variance but may increase computational cost.\n",
    "# - Optimal folds balance bias-variance tradeoff while minimizing computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y2ZBM7bXEhmjS6AlDuqwt",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 7 (medium): Reflection on resultsüí°**\n",
    "1. Use the plotted mean and variance to argue about the model performance. \n",
    "2. List reasons for the variability in model performance? \n",
    "3. Compare the variability in model perfomance observed in the tutorial with the results of the current exercise.\n",
    "4. Argue how the regularized model performs compared to the standard linear regression implemented in the tutorial. Print the model parameters and use them to argue for differences between the linear model and the regularized model.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U4rBl-9hNh4V9y1zhF_Pw",
   "metadata": {},
   "source": [
    "## Task 7 reflection\n",
    "\n",
    "\n",
    "#### **1. Argument on Model Performance**\n",
    "- The plotted mean \\( R^2 \\) scores indicate that the model generalizes well for moderate regularization parameters (\\( \\lambda \\)).\n",
    "- The variance of \\( R^2 \\) scores decreases with more folds, suggesting better stability in performance when the training and testing sets are balanced.\n",
    "\n",
    "#### **2. Reasons for Variability**\n",
    "- **Small folds (e.g., 2):** Larger variance due to more significant differences in training/testing splits.\n",
    "- **High folds (e.g., 20):** Lower variance but potential overfitting as training sets become smaller.\n",
    "- Data characteristics, such as noise and multicollinearity, contribute to variability.\n",
    "- Inconsistent feature scaling without normalization can also lead to performance differences.\n",
    "\n",
    "#### **3. Comparison with Tutorial**\n",
    "- In the tutorial, standard linear regression showed higher variability due to lack of regularization.\n",
    "- Regularization in this exercise (Ridge regression) reduces variability by penalizing large coefficients, leading to more stable results.\n",
    "\n",
    "#### **4. Comparison of Regularized vs Linear Regression**\n",
    "- **Standard Linear Regression:**\n",
    "  - Prone to overfitting, especially with higher polynomial degrees.\n",
    "  - Coefficients can become very large, leading to high variance in predictions.\n",
    "- **Regularized Model (Ridge Regression):**\n",
    "  - Shrinks coefficients, reducing overfitting.\n",
    "  - Performs better on unseen data due to improved generalization.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optionnal code, most likely remove\n",
    "\n",
    "# Example code to compare coefficients\n",
    "linear_model = Pipeline([\n",
    "    (\"features\", PolynomialFeatures(3)),\n",
    "    (\"normalization\", Normalizer()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "ridge_model = Pipeline([\n",
    "    (\"features\", PolynomialFeatures(3)),\n",
    "    (\"normalization\", Normalizer()),\n",
    "    (\"model\", Ridge(alpha=optimal_alpha))  # Optimal alpha from Task 5\n",
    "])\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Linear Model Coefficients:\", linear_model.named_steps['model'].coef_)\n",
    "print(\"Ridge Model Coefficients:\", ridge_model.named_steps['model'].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a912f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
